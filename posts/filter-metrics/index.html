<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Let's filter Prometheus metrics exposed by Softwares | Polyedre</title>
<meta name=keywords content><meta name=description content="This post discusses about the limit of Prometheus crawlers when a software exposes too much metrics, and provides a solution to limit the number of metrics exported by Softwares that cannot be modified.
Sometimes you&rsquo;re just crawling too much The number of metrics exported on the /metrics endpoint of a Software can impact the performances of Prometheus crawlers. There is at least two ways crawlers could fail to fetch metrics for a software."><meta name=author content="Polyedre"><link rel=canonical href=https://polyedre.github.io/posts/filter-metrics/><link crossorigin=anonymous href=/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css integrity="sha256-yIlj/i15RiAA/Q+xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as=style><link rel=preload href=/profile.jpeg as=image><script defer crossorigin=anonymous src=/assets/js/highlight.min.2eadbb982468c11a433a3e291f01326f2ba43f065e256bf792dbd79640a92316.js integrity="sha256-Lq27mCRowRpDOj4pHwEybyukPwZeJWv3ktvXlkCpIxY=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://polyedre.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://polyedre.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://polyedre.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://polyedre.github.io/apple-touch-icon.png><link rel=mask-icon href=https://polyedre.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.124.1"><link rel=alternate hreflang=en href=https://polyedre.github.io/posts/filter-metrics/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-PL8DXXK8VN"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-PL8DXXK8VN",{anonymize_ip:!1})}</script><meta property="og:title" content="Let's filter Prometheus metrics exposed by Softwares"><meta property="og:description" content="This post discusses about the limit of Prometheus crawlers when a software exposes too much metrics, and provides a solution to limit the number of metrics exported by Softwares that cannot be modified.
Sometimes you&rsquo;re just crawling too much The number of metrics exported on the /metrics endpoint of a Software can impact the performances of Prometheus crawlers. There is at least two ways crawlers could fail to fetch metrics for a software."><meta property="og:type" content="article"><meta property="og:url" content="https://polyedre.github.io/posts/filter-metrics/"><meta property="og:image" content="https://polyedre.github.io/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-10-19T15:41:17+01:00"><meta property="article:modified_time" content="2023-10-19T15:41:17+01:00"><meta property="og:site_name" content="Polyedre"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://polyedre.github.io/"><meta name=twitter:title content="Let's filter Prometheus metrics exposed by Softwares"><meta name=twitter:description content="This post discusses about the limit of Prometheus crawlers when a software exposes too much metrics, and provides a solution to limit the number of metrics exported by Softwares that cannot be modified.
Sometimes you&rsquo;re just crawling too much The number of metrics exported on the /metrics endpoint of a Software can impact the performances of Prometheus crawlers. There is at least two ways crawlers could fail to fetch metrics for a software."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://polyedre.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Let's filter Prometheus metrics exposed by Softwares","item":"https://polyedre.github.io/posts/filter-metrics/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Let's filter Prometheus metrics exposed by Softwares","name":"Let\u0027s filter Prometheus metrics exposed by Softwares","description":"This post discusses about the limit of Prometheus crawlers when a software exposes too much metrics, and provides a solution to limit the number of metrics exported by Softwares that cannot be modified.\nSometimes you\u0026rsquo;re just crawling too much The number of metrics exported on the /metrics endpoint of a Software can impact the performances of Prometheus crawlers. There is at least two ways crawlers could fail to fetch metrics for a software.","keywords":[],"articleBody":"This post discusses about the limit of Prometheus crawlers when a software exposes too much metrics, and provides a solution to limit the number of metrics exported by Softwares that cannot be modified.\nSometimes you’re just crawling too much The number of metrics exported on the /metrics endpoint of a Software can impact the performances of Prometheus crawlers. There is at least two ways crawlers could fail to fetch metrics for a software.\nThe first case involve a software that is deployed with a small number of Pods, but exposes a lot of metrics. This is the case of Kube State Metrics for instance. For each Kubernetes ressource present on the cluster, Kube State Metrics will expose multiple metrics. On big Kubernetes clusters, the number of metrics could exceed 1 million metrics. This means that Prometheus crawlers need to fetch this million of lines every 20 seconds (depends on the interval configured). With one million lines, the body of the request could exceed 80MB.\nHere, the limitation is the time required to receive all the lines. If the duration of the HTTP request exceed the period between two crawls, the crawlers timeouts the HTTP request and starts a new one. As a consequence, you may lose your metrics.\nThe second case involve a software that exported a reasonable number of metrics, but is deployed with a lot of Pods. Because the crawler crawl each Pod individually, each HTTP request finish quickly.\nIn this second case, the limit could be the CPU allocated to the crawler, or any intermediate Pod that aggregates the metrics. If prometheus is not able to process all the metrics quickly enough, you may lose your metrics.\nThe simple solution would be to throw more CPU at the problem. But let’s be honest, a better solution might be to check if all the metrics exported are useful. You might save some storage space too!\nIdentify metrics that can do not need to be exported In order to evaluate which metrics can be filtered, you need to:\nknow all the metrics that are exported by Pods, and the cardinality of each metrics; decide which metrics can be dropped. The easiest way to know which metrics are exported by a Pod is to send a GET HTTP request to the /metrics endpoint of the Pod. This can be achieved with curl and kubectl port-forward if you want to send the request from your laptop, or with curl installed inside a Pod. I will not cover how to use these tools here.\nYou can use some shell tools to sort which metrics have the highest cardinality:\ncurl https://ENDPOINT/metrics | grep -v '#' | cut -d'{' -f1 | uniq -c | sort -h Ideally, you want to focus your efforts on high cardinality metrics.\nReduce the number of metrics exported There is three possibility know:\nYou can modify the software that exports the metrics to stop exporting useless metrics, or to reduce the cardinality of those. You cannot modify the software, but there is a builtin way to stop some metrics from being exported. You’re out of luck: you cannot either modify the software or modify its configuration so that less metrics are exported. The end of this post will focus on the third option.\nIf the software is Open Source, you could patch it to remove the metrics. However, this may come as a cost, because now, you will need to maintain your patch.\nInstead, the solution I’m proposing is to run a HTTP proxy as a SideCar container that filters metrics exposed by the software. With this solution, you do not need to rebuild the docker image. You “just” have to modify the Pod definition.\nI implemented a simple HTTP proxy that targets a single HTTP endpoint, and removes all the lines that match a regular expression. One fear that I had with this solution was that this introduces some latency, because the proxy need to run the HTTP request, then serve the response with the lines filtered out. As it turns out, most of the HTTP time used to serve the HTTP request comes from the network, so as long as the proxy removes enough lines in the body response, a HTTP request to /metrics could be instead faster.\nThe tool is called metrics-filter.\n","wordCount":"716","inLanguage":"en","datePublished":"2023-10-19T15:41:17+01:00","dateModified":"2023-10-19T15:41:17+01:00","author":{"@type":"Person","name":"Polyedre"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://polyedre.github.io/posts/filter-metrics/"},"publisher":{"@type":"Organization","name":"Polyedre","logo":{"@type":"ImageObject","url":"https://polyedre.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://polyedre.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://polyedre.github.io/profile.jpeg alt=logo aria-label=logo height=35>Home</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></span></div><ul id=menu><li><a href=https://polyedre.github.io/posts/ title=Posts><span>Posts</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://polyedre.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://polyedre.github.io/posts/>Posts</a></div><h1 class=post-title>Let's filter Prometheus metrics exposed by Softwares</h1><div class=post-meta>&lt;span title='2023-10-19 15:41:17 +0100 +0100'>October 19, 2023&lt;/span>&amp;nbsp;·&amp;nbsp;4 min&amp;nbsp;·&amp;nbsp;Polyedre&nbsp;|&nbsp;<a href=https://github.com/polyedre/polyedre.github.io/content/posts/filter-metrics.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><p>This post discusses about the limit of Prometheus crawlers when a software
exposes too much metrics, and provides a solution to limit the number of metrics
exported by Softwares that cannot be modified.</p><h2 id=sometimes-youre-just-crawling-too-much>Sometimes you&rsquo;re just crawling too much<a hidden class=anchor aria-hidden=true href=#sometimes-youre-just-crawling-too-much>#</a></h2><p>The number of metrics exported on the <code>/metrics</code> endpoint of a Software can
impact the performances of Prometheus crawlers. There is at least two ways
crawlers could fail to fetch metrics for a software.</p><p>The first case involve a software that is deployed with a small number of Pods,
but exposes a lot of metrics. This is the case of Kube State Metrics for
instance. For each Kubernetes ressource present on the cluster, Kube State
Metrics will expose multiple metrics. On big Kubernetes clusters, the number of
metrics could exceed 1 million metrics. This means that Prometheus crawlers need
to fetch this million of lines every 20 seconds (depends on the interval
configured). With one million lines, the body of the request could exceed 80MB.</p><p>Here, the limitation is the time required to receive all the lines. If the
duration of the HTTP request exceed the period between two crawls, the crawlers
timeouts the HTTP request and starts a new one. As a consequence, you may lose
your metrics.</p><p>The second case involve a software that exported a reasonable number of metrics,
but is deployed with a lot of Pods. Because the crawler crawl each Pod
individually, each HTTP request finish quickly.</p><p>In this second case, the limit could be the CPU allocated to the crawler, or any
intermediate Pod that aggregates the metrics. If prometheus is not able to
process all the metrics quickly enough, you may lose your metrics.</p><p>The simple solution would be to throw more CPU at the problem. But let&rsquo;s be
honest, a better solution might be to check if all the metrics exported are
useful. You might save some storage space too!</p><h2 id=identify-metrics-that-can-do-not-need-to-be-exported>Identify metrics that can do not need to be exported<a hidden class=anchor aria-hidden=true href=#identify-metrics-that-can-do-not-need-to-be-exported>#</a></h2><p>In order to evaluate which metrics can be filtered, you need to:</p><ol><li>know all the metrics that are exported by Pods, and the cardinality of each
metrics;</li><li>decide which metrics can be dropped.</li></ol><p>The easiest way to know which metrics are exported by a Pod is to send a GET
HTTP request to the /metrics endpoint of the Pod. This can be achieved with
<code>curl</code> and <code>kubectl port-forward</code> if you want to send the request from your
laptop, or with <code>curl</code> installed inside a Pod. I will not cover how to use these
tools here.</p><p>You can use some shell tools to sort which metrics have the highest cardinality:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Bash data-lang=Bash><span style=display:flex><span>curl https://ENDPOINT/metrics | grep -v <span style=color:#e6db74>&#39;#&#39;</span> | cut -d<span style=color:#e6db74>&#39;{&#39;</span> -f1 | uniq -c | sort -h
</span></span></code></pre></div><p>Ideally, you want to focus your efforts on high cardinality metrics.</p><h2 id=reduce-the-number-of-metrics-exported>Reduce the number of metrics exported<a hidden class=anchor aria-hidden=true href=#reduce-the-number-of-metrics-exported>#</a></h2><p>There is three possibility know:</p><ol><li>You can modify the software that exports the metrics to stop exporting
useless metrics, or to reduce the cardinality of those.</li><li>You cannot modify the software, but there is a builtin way to stop some
metrics from being exported.</li><li>You&rsquo;re out of luck: you cannot either modify the software or modify its
configuration so that less metrics are exported.</li></ol><p>The end of this post will focus on the third option.</p><p>If the software is Open Source, you could patch it to remove the metrics.
However, this may come as a cost, because now, you will need to maintain your
patch.</p><p>Instead, the solution I&rsquo;m proposing is to run a HTTP proxy as a SideCar
container that filters metrics exposed by the software. With this solution, you
do not need to rebuild the docker image. You &ldquo;just&rdquo; have to modify the Pod
definition.</p><p>I implemented a simple HTTP proxy that targets a single HTTP endpoint, and
removes all the lines that match a regular expression. One fear that I had with
this solution was that this introduces some latency, because the proxy need to
run the HTTP request, then serve the response with the lines filtered out. As it
turns out, most of the HTTP time used to serve the HTTP request comes from the
network, so as long as the proxy removes enough lines in the body response, a
HTTP request to <code>/metrics</code> could be instead <strong>faster</strong>.</p><p>The tool is called <a href=https://github.com/polyedre/metrics-filter>metrics-filter</a>.</p></div><footer class=post-footer><nav class=paginav><a class=next href=https://polyedre.github.io/posts/ktx/><span class=title>Next Page »</span><br><span>Using multiple kubectl contexts at the same time</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://polyedre.github.io/>Polyedre</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>